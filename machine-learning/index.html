<!DOCTYPE html>
<html lang="ru">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Блог Павла Попова</title>
  <meta name="title" content="Machine Learning">
  <meta name="description" content="Павел Попов">

  <meta property="og:url" content="/machine-learning/">
  <meta property="og:title" content="Machine Learning">
  
  <meta property="og:image" content="https://d1z850dzhxs7de.cloudfront.net/topics/ml/large-icon.png">
  
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Павел Попов">
  <link rel="icon" type="image/png" href="/public/favicon.ico">
  <link rel="alternate" type="application/rss+xml" title="Блог Павла Попова" href="/blog-feed.xml">
  <link href="http://fonts.googleapis.com/css?family=PT+Serif:400,700,400italic|PT+Sans:400,700,400italic|Roboto:100,300|Roboto+Slab:300,400&amp;subset=latin,cyrillic" rel="stylesheet">
  <link href="/public/css/main.css" rel="stylesheet">

  <!-- CSS -->
  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Custom -->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min.js"></script>
  <link  href="http://fotorama.s3.amazonaws.com/4.6.0/fotorama.css" rel="stylesheet"> <!-- 3 KB -->
<script src="http://fotorama.s3.amazonaws.com/4.6.0/fotorama.js"></script> <!-- 16 KB -->
  <script src="/public/js/dug.js"></script>
  <script src="/public/js/application.js"></script>
</head>

<body>
<header class="row-1">
  <nav class="title-nav" role="navigation">
    <h1 class="title-nav-header"><a href="/">Павел Попов</a>
      <span class="title-nav-slash">/</span>
      <a href="/">Блог</a>
    </h1>
  </nav>
</header>

  <script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<section class="single-entry row-1" role="main">
  <article class="entry">
    
  
  
  




  <header class="entry-header">
    <h1 class="entry-title">
      <a href="/machine-learning/">Machine Learning</a>
    </h1>
  </header>

  <div class="entry-body content-area">
    
      <div class="entry-cover">
        <img src="https://d1z850dzhxs7de.cloudfront.net/topics/ml/large-icon.png" alt="Machine Learning">
      </div>
    

    <p>Записался на курс <a href="https://www.coursera.org/course/ml">Machine Learning</a> на Coursera. Попробую целиком
прослушать и сдать экзамен.</p>

<p>Здесь буду записывать очень краткие конспекты лекций.</p>

<!--more-->

<h2 id="week-1">Week 1</h2>

<p>Определение ML:</p>

<blockquote>
  <p>A computer program is said to learn from experience E, with respect to some
task T, and some performance measure P, if its performance on T as measured
by P improves with experience E.</p>

  <p><em>Tom Mitchell</em></p>
</blockquote>

<p>Задачи, решаемые ML:</p>

<ul>
  <li>Data mining во всех его проявлениях – обработка медицинских данных, данных
соцсетей, предпочтения в интернет-магазинах, контекстная реклама</li>
  <li>Приложения, которые невозможно написать – автономный вертолет, распознование
символов, Natural Language Processing, компьютерное зрение.</li>
</ul>

<p>Два типа ML:</p>

<ul>
  <li>Supervised learning</li>
  <li>Unsupervised learning</li>
</ul>

<h3 id="supervised-learning">Supervised Learning</h3>

<p>Supervised Learning – это когда у нас есть набор данных E, по которым
<em>известен</em> результат решения задачи T. Стоит задача для <em>новых</em> данных получить
результат.</p>

<p>Примеры:</p>

<ul>
  <li>Регрессия – есть факты по стоимости квадратного метра жилья в зависимости от
площади. Требуется для нового значения площади определить стоимость.</li>
  <li>Спам-фильтр – обучаем машину вручную отмечая спам-письма, дальше она сама
учится помечать спам</li>
  <li>Медицинские данные (например, по факту злокачественной раковой опухоли груди)
– знаем, факты, когда для определенного объема опухоли и возраста пациента
опухоль была злокачественной или нет, задача - для нового пациента определить
тип опухоли</li>
</ul>

<p>Первый пример – Regression Problem, два последних примера – пример
Classification Problem. В Classification Problem стоит задача по <em>в принципе
бесконечному</em> количеству результатов проклассифицировать данные по какому-то
принципу.</p>

<h3 id="unsupervised-learning">Unsupervised Learning</h3>

<p>Unsupervised Learning – принцип тот же, что и в Supervised Learning, только
результата “правильных” значений нет. Пример задач – разбиение данных на
группы, выделение фрагментов.</p>

<p>Человек визуально эту задачу решает очень хорошо – мозг настроен на выделение
паттернов. Задача – научить этому машину.</p>

<p>Примеры:</p>

<ul>
  <li>Сегментация клиентов бизнеса по группам</li>
  <li>Анализ данных соц. сетей</li>
  <li>Группировка разных новостей из разных источников в топики – Google News</li>
  <li>Геном человека – нужно выделить группы генов, которые есть у разных типов
людей</li>
  <li>Астрономические данные – выделение туманностей и галактик</li>
  <li>Разделение дорожек в аудио данных</li>
</ul>

<h3 id="training-set">Training Set</h3>

<p>В случае Supervised Learning исходный набор данных с решенной задачей
называется Training Set. В самом простом случае регрессии это двумерный набор
данных \((x, y)\).</p>

<p>Training Set подается в Learning Algorithm и на выходе получается гипотеза
(Hypothesis) h. Эта “гипотеза” и есть обученный алгоритм, что если мы на вход
подадим какое-то новое \(x’\), отсутствующее в Training Set, то получим
результат работы нашей модели.</p>

<pre><code>h :: x -&gt; y
map h [x] = [y]
</code></pre>

<h2 id="week-2">Week 2</h2>

<h3 id="section">Регрессия</h3>

<p>Самый простой случай решения задачи регрессии – линейная регрессия. В этом
случае функцией \(h\) является прямая.</p>

<div>

\[ h(x) = \theta_{0} + \theta_{1} x \]

</div>

<p>В этом случае, задача сводится к поиску оптимальных коэффициентов прямой.</p>

<p>Задача имеет \(n\) features (параметров, но это слово уже занято для
\(\theta_j\)) и \(m\) наборов данных в тренировочном наборе.</p>

<p>Конструируют некоторую функцию веса ошибки \(J(\Theta)\) от вектора
параметров размерностью \(n+1\):</p>

<div>

\[\Theta = [\theta_0, \theta_1, ..., \theta_n] \]

</div>

<p>Обычно это среднеквадратичное отклонение:</p>

<div>

\[ J(\Theta) = \frac{1}{2m} \sum_{i=0}^m \left(h_{\theta}(x^{(i)}) - y^{(i)}\right)^2 \]

</div>

<p>Т.к. параметров, определяющих алгоритм может быть множество (в качестве примера
– зависимость стоимости дома от площади, этажа, года постройки, количества
спален), удобно пользоваться аппаратом линейной алгебры – исходные наборы
представляются в виде матрицы \(X\), а параметры регрессии – в виде вектора
\(\Theta\). Тогда, положив \(x_0 = 1\), конкретное значение получается в
виде:</p>

<div>

\[ h_\theta(x) = \Theta^T x \]

</div>

<p>Задачей является поиск таких параметров \(\Theta\), для которых вес ошибки
будет минимальным.</p>

<p>Минимум функции веса ошибки \(J(\Theta)\) ищут с помощью градиентного спуска
или с помощью явного аналитического решения с помощью линейной алгебры.</p>

<h3 id="gradient-descent">Градиентный спуск (Gradient descent)</h3>

<p>Выбираем некое начальное значение параметров. Одновременно вычисляем новые
значения для всех \(\theta_j\):</p>

<div>

\[ \theta_j = \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\Theta) \]

</div>

<p>до тех пор, пока изменение \(J(\Theta)\) не упадет до малого значения (обычно
– 0.001).</p>

<p>Здесь \(\alpha\) – параметр скорости обучения. Если его взять слишком
большим, то спуск пропустит локальный минимум и функция веса ошибки будет
расходящейся. Если его взять слишком маленьким, то спуск будет очень медленным.
Оптимально начинать с некотого начального значения, например, 0.01, и строить
зависимость функции ошибки от количества итерация. Дальше подбирать параметры
исходя от вида графика этой функции. Расходится – уменьшаем (удобно – в 3
раза), сходится медленно – увеличиваем.</p>

<p>Чтобы была скорость работы градиентного спуска была оптимальной,
рекомендуется провести нормализацию и усреднение значений features, чтобы все
они находились в интервале \(-1 &lt;= x &lt;= 1\). Для нормализации значений
используется формула:</p>

<div>

\[ x_j^{(i)} = \frac{x_j^{(i)} - \mu^{(i)}}{ \sigma^{(i)} } \]

</div>

<p>Здесь \(\mu^{(i)}\) – среднее по всем \(x^{(i)}\), \(\sigma^{(i)}\) –
стандартное отклонение по всем \(x^{(i)}\). В дальнейшем, при задании
расчетных значений их также необходимо нормализовывать по такой же формуле. При
этом результирующие значения \( y \) не надо нормализовать.</p>

<p>В случае линейной регрессии можно аналитически вычислить частные производные
функции ошибки по параметру и использовать результат в явном виде, чтобы не
производить численный расчет производной.</p>

<div>

\[ \theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=0}^{m} \left ( h_\theta (x^{(i)}) - y^{(i)} \right ) x_j^{(i)} \]

</div>

<h3 id="section-1">Решение в аналитическом виде</h3>

<p>Глобальный минимум для функции ошибки \(J(\Theta)\) – точка, в которой все
частичные производные по компонентам \(\Theta\) равны нулю. Исходя из этого,
можно аналитически вычислить оптимальное значение \(\Theta\). Для случая
линейной регрессии получается следующая формула:</p>

<div>

\[ \Theta = (X^T X)^{-1}X^Ty \]

</div>

<p>Здесь \(X\) – матрица размера \(m \times n+1 \) с исходными данными из
обучающего набора, \(y\) – вектор с результатами обучающего набора.</p>

<p>Сложность расчета здесь в вычислении компонента \((X^T X)^{-1}\). Также может
оказаться, что для этой матрицы нет обратной. Такое обычно случается если среди
features есть линейно зависимые друг от друга (например, площадь дома в
квадратных футах и площать дома в квадратных метрах). В этом случае надо
исключить одну из таких features.</p>

<p>В случае решения в аналитическом виде нет необходимости проводить нормализацию и
усреднение данных из обучающего набора.</p>

<p>С современными мощностями можно использовать решение в аналитическом виде вплоть
до 10000 features и строк в обучающем наборе. При большем количестве имеет смысл
использовать градиентный спуск. При этом количество строк в обучающем наборе
оказывается неважным – перемножение \(X^T X\) это перемножение матриц
размерностей \( n \times m \) и \( m \times n \), так что в результате
получается матрица размера \( n \times n \), для которой необходимо вычислить
обратную матрицу.</p>

<h3 id="matlab">Эксперименты с MATLAB</h3>

<p>После выполнения задания по 1-2 неделям я поигрался с MATLAB для лучшего
понимания материала. Результаты работы скрипта можно посмотреть
<a href="/pages/ml/linear_regression/regression.html">здесь</a>.</p>

<h2 id="week-3">Week 3</h2>

<h3 id="section-2">Логистическая регрессия</h3>

<p>Применяется в classification problems (классификация фактов к одному из двух
классов). Примеры - пометка почты как спам, определение злокачественности
опухоли. Для начала рассмотрим бинарную классификацию, а потом расширим на
multiclass classification.</p>

<p>В бинарной классификации значения результата \( y \in (0,1) \). Линейная
регрессия в данном случае не очень подходит, т.к. есть сильная зависимость от
значений в training set и значение функции может быть гораздо больше 1.</p>

<p>Удобно использовать в качестве гипотезы логистическую регрессию:</p>

<div>

\[ h_{\theta}(x) = g(\Theta^T x) \]
\[ g(z) = \frac{1}{1-e^{-z}} \]

</div>

<p>В этом случае \( 0 &lt;= h_{\theta}(x) &lt;= 1 \), и мы можем рассматривать значение
функции \( h \) в качестве вероятности положительного результата. Если \(
h_{\theta}(x) &gt;= 0.5 \), то \( y = 1 \), иначе \( y = 0 \).</p>

<p><img src="/assets/ml/sigmoid.png" alt="sigmoid function" /></p>

<p>Видно, что \( y = 1 \) когда \( h_{\theta}(x) &gt;= 0.5, g(z) &gt;= 0.5 \), т.е.
когда \( \Theta^T x &gt; 0 \). Граница \( \Theta^T x = 0 \) является decision
boundary. По одну сторону от этой границы располагаются разные классы решения.</p>

<p>Граница решения это свойство не training set но функции гипотезы - \(
h_{\theta}(x) \). В общем случае граница не обязательно прямая (с линейными
членами \( x^{(i)} \), можно добавить различные квадратичные члены в
hypothesis function, например чтобы получить границу в виде окружности:</p>

<div>

\[ h_{\theta}(x) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2 +\theta_4 x_2^2) \]

</div>

<p>Если выбрать</p>

<div>

\[ \Theta =
    \begin{bmatrix}
    -1\\
    0\\
    0\\
    1\\
    1
    \end{bmatrix}
 \]

</div>

<p>то границей будет окружность \( x_1^2+x_2^2 = 1 \).</p>



    
  </div>

  
  <footer class="entry-footer">
    <time class="entry-timestamp">08 Mar 2014</time>
  </footer>
  

  </article>
</section>



  <!-- Yandex.Metrika counter -->
<script type="text/javascript">
(function (d, w, c) {
    (w[c] = w[c] || []).push(function() {
        try {
            w.yaCounter21201856 = new Ya.Metrika({id:21201856,
                    clickmap:true,
                    trackLinks:true,
                    accurateTrackBounce:true});
        } catch(e) { }
    });

    var n = d.getElementsByTagName("script")[0],
        s = d.createElement("script"),
        f = function () { n.parentNode.insertBefore(s, n); };
    s.type = "text/javascript";
    s.async = true;
    s.src = (d.location.protocol == "https:" ? "https:" : "http:") + "//mc.yandex.ru/metrika/watch.js";

    if (w.opera == "[object Opera]") {
        d.addEventListener("DOMContentLoaded", f, false);
    } else { f(); }
})(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="//mc.yandex.ru/watch/21201856" style="position:absolute; left:-9999px;" alt="" /></div>
</noscript>
<!-- /Yandex.Metrika counter -->

</body>
</html>
